{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Counting Words in Homer\n\nWord frequency analysis of Homer's *Iliad* and *Odyssey* using the Greek Tesserae corpus and `GreekTesseraeReader`.\n\n## Setup\n\n```bash\n# Install latincy-readers\npip install latincy-readers\n\n# Install OdyCy model (for lemmatization and POS tagging)\npip install https://huggingface.co/chcaa/grc_odycy_joint_sm/resolve/main/grc_odycy_joint_sm-any-py3-none-any.whl\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "from latincyreaders import GreekTesseraeReader, AnnotationLevel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Greek Tesserae Corpus\n",
    "\n",
    "The corpus will be downloaded automatically on first use from the [CLTK Greek Tesserae repository](https://github.com/cltk/grc_text_tesserae)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use TOKENIZE for fast iteration (no OdyCy model needed)\n",
    "reader = GreekTesseraeReader(annotation_level=AnnotationLevel.TOKENIZE)\n",
    "\n",
    "# List available files\n",
    "all_files = reader.fileids()\n",
    "print(f\"Total files: {len(all_files)}\")\n",
    "print(\"\\nHomer files:\")\n",
    "homer_files = reader.fileids(match=r\"homer\")\n",
    "for f in homer_files:\n",
    "    print(f\"  {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Frequency with `texts_by_line()`\n",
    "\n",
    "The fastest approach: iterate over raw text lines with zero NLP overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple word counting from raw text\n",
    "word_counts = Counter()\n",
    "\n",
    "for citation, text in reader.texts_by_line(fileids=homer_files):\n",
    "    words = text.split()\n",
    "    word_counts.update(words)\n",
    "\n",
    "print(f\"Total word tokens: {sum(word_counts.values())}\")\n",
    "print(f\"Unique word types: {len(word_counts)}\")\n",
    "print(\"\\nMost common words:\")\n",
    "for word, count in word_counts.most_common(20):\n",
    "    print(f\"  {word:20s} {count:>6d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search with `search()` and `find_lines()`\n",
    "\n",
    "Find specific words and patterns across the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for Achilles (various forms)\n",
    "print(\"Lines mentioning Achilles:\")\n",
    "for fileid, citation, text, matches in reader.search(r\"Ἀχιλ\", fileids=homer_files):\n",
    "    print(f\"  {citation}: {text[:80]}...\")\n",
    "    if len(list(reader.search(r\"Ἀχιλ\", fileids=homer_files))) > 10:\n",
    "        print(\"  ...\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrences of key characters\n",
    "characters = {\n",
    "    \"Achilles\": r\"Ἀχιλ\",\n",
    "    \"Hector\": r\"Ἕκτ\",\n",
    "    \"Odysseus\": r\"Ὀδυσ\",\n",
    "    \"Zeus\": r\"Ζε[υύ]\",\n",
    "    \"Athena\": r\"Ἀθην\",\n",
    "}\n",
    "\n",
    "print(\"Character mentions in Homer:\")\n",
    "for name, pattern in characters.items():\n",
    "    count = len(list(reader.search(pattern, fileids=homer_files)))\n",
    "    print(f\"  {name:15s} {count:>4d} lines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KWIC (Keyword in Context)\n",
    "\n",
    "See words in their surrounding context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KWIC for a key term\n",
    "print(\"KWIC for 'μῆνιν' (wrath):\")\n",
    "for hit in reader.kwic(\"μῆνιν\", fileids=homer_files, window=5, limit=10):\n",
    "    print(f\"  {hit['left']:>40s} [{hit['match']}] {hit['right']:<40s}\")\n",
    "    print(f\"  {'':>40s}  {hit['citation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams\n",
    "\n",
    "Extract bigrams and trigrams for collocational analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigram frequency\n",
    "bigram_counts = Counter(reader.ngrams(n=2, fileids=homer_files))\n",
    "\n",
    "print(\"Most common bigrams:\")\n",
    "for bigram, count in bigram_counts.most_common(15):\n",
    "    print(f\"  {bigram:30s} {count:>4d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Concordance (with OdyCy)\n\nBuild a concordance keyed by lemma. This requires the OdyCy model for lemmatization.\n\n```bash\npip install https://huggingface.co/chcaa/grc_odycy_joint_sm/resolve/main/grc_odycy_joint_sm-any-py3-none-any.whl\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload with BASIC annotation level for lemmatization\n",
    "reader_nlp = GreekTesseraeReader(annotation_level=AnnotationLevel.BASIC)\n",
    "\n",
    "# Build concordance for Homer\n",
    "conc = reader_nlp.concordance(fileids=homer_files, basis=\"lemma\")\n",
    "\n",
    "print(f\"Unique lemmas: {len(conc)}\")\n",
    "print(\"\\nMost cited lemmas:\")\n",
    "top_lemmas = sorted(conc.items(), key=lambda x: len(x[1]), reverse=True)[:15]\n",
    "for lemma, citations in top_lemmas:\n",
    "    print(f\"  {lemma:20s} {len(citations):>4d} occurrences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results\n",
    "\n",
    "Export search results for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export search results as TSV\n",
    "results = reader.find_sents(pattern=r\"Ἀχιλ\", fileids=homer_files)\n",
    "tsv = reader.export_search_results(results, format=\"tsv\")\n",
    "print(tsv[:500])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}