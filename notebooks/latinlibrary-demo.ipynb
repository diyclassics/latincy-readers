{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# LatinLibrary Reader Demo\n",
    "\n",
    "This notebook demonstrates the `LatinLibraryReader` for working with the [Latin Library](https://thelatinlibrary.com/) corpus.\n",
    "\n",
    "**Key differences from TesseraeReader:**\n",
    "- Plain text files (`.txt`) instead of citation-annotated `.tess` files\n",
    "- No built-in citation system (uses `fileid:sentN` fallback)\n",
    "- Paragraph extraction via `paras()` method\n",
    "- Title metadata extracted from first line of each file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "\n",
    "from latincyreaders import LatinLibraryReader, AnnotationLevel\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up reader\n",
    "\n",
    "# Auto-downloads corpus on first use if not found\n",
    "L = LatinLibraryReader()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fileids-header",
   "metadata": {},
   "source": [
    "## Fileids and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## First 10 filenames\n",
    "\n",
    "files = L.fileids()[:10]\n",
    "pprint(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get files by pattern match (regex)\n",
    "files = L.fileids(match='horace')\n",
    "pprint(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get files by pattern - supports regex\n",
    "files = L.fileids(match=r'vergil.*aen')\n",
    "pprint(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get files by partial match\n",
    "files = L.fileids(match='cicero')[:10]\n",
    "pprint(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case-insensitive regex matching\n",
    "files = L.fileids(match=r'ovid')[:15]\n",
    "pprint(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all files\n",
    "all_files = L.fileids()\n",
    "print(f\"Total files: {len(all_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metadata-header",
   "metadata": {},
   "source": [
    "### Metadata\n",
    "\n",
    "LatinLibrary files have minimal metadata compared to Tesserae. The `title` is extracted from the first line of each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-meta-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get metadata for a specific file\n",
    "# Note: LatinLibrary extracts title from first line\n",
    "\n",
    "sample_file = L.fileids(match='cicero')[0]\n",
    "meta = L.get_metadata(sample_file)\n",
    "\n",
    "print(f\"Metadata for {sample_file}:\")\n",
    "pprint(meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-meta-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through metadata\n",
    "for fileid, meta in list(L.metadata())[:5]:\n",
    "    title = meta.get('title', 'No title')\n",
    "    print(f\"{fileid}: {title[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "docs-header",
   "metadata": {},
   "source": [
    "## Doc structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a file to work with\n",
    "catullus_files = L.fileids(match='catullus')\n",
    "print(f\"Catullus files: {catullus_files}\")\n",
    "catullus = catullus_files[0] if catullus_files else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Docs - spaCy Doc objects with NLP annotations\n",
    "\n",
    "if catullus:\n",
    "    catullus_doc = next(L.docs(catullus))\n",
    "    print(catullus_doc[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Texts - raw strings (zero NLP overhead)\n",
    "\n",
    "if catullus:\n",
    "    catullus_text = next(L.texts(catullus))\n",
    "    pprint(catullus_text[:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "no-docrows",
   "metadata": {},
   "source": [
    "### Note: No doc_rows() or lines()\n",
    "\n",
    "Unlike TesseraeReader, LatinLibraryReader does not have citation-annotated lines. Use `sents()` or `paras()` instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "units-header",
   "metadata": {},
   "source": [
    "## Doc units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a Cicero file for examples\n",
    "cicero_files = L.fileids(match='cicero')\n",
    "catilinam = cicero_files[0] if cicero_files else None\n",
    "print(f\"Using file: {catilinam}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Paras - paragraph spans (available in LatinLibrary!)\n",
    "\n",
    "if catilinam:\n",
    "    paras = list(L.paras(catilinam))\n",
    "    print(f\"Total paragraphs: {len(paras)}\")\n",
    "    print()\n",
    "    for i, para in enumerate(paras[:3]):\n",
    "        print(f\"Para {i+1}: {para.text[:100]}...\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sents - spaCy Span objects\n",
    "\n",
    "if catilinam:\n",
    "    sents = L.sents(catilinam)\n",
    "    for i in range(1, 6):\n",
    "        print(f'Sent {i}: {next(sents)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokens - spaCy Token objects\n",
    "\n",
    "if catilinam:\n",
    "    tokens = L.tokens(catilinam)\n",
    "    for i in range(1, 10):\n",
    "        print(f'Word {i}: {next(tokens)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token linguistic attributes (BASIC level: text, lemma, POS, tag)\n",
    "\n",
    "if catilinam:\n",
    "    tokens = L.tokens(catilinam)\n",
    "    t = next(tokens)\n",
    "    print(f\"text: {t.text}, lemma: {t.lemma_}, pos: {t.pos_}, tag: {t.tag_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For custom text processing, work with the raw text or spaCy Doc\n",
    "\n",
    "if catilinam:\n",
    "    # Get text as strings\n",
    "    for token_text in L.tokens(catilinam, as_text=True):\n",
    "        processed = token_text.lower()\n",
    "        print(processed, end=' ')\n",
    "        break  # Just show first token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenized sents - use spaCy directly\n",
    "# Get (token, lemma, tag) tuples from sentences\n",
    "\n",
    "if catilinam:\n",
    "    sents = L.sents(catilinam)\n",
    "    for i in range(1, 4):\n",
    "        sent = next(sents)\n",
    "        tok_sent = [(t.text, t.lemma_, t.tag_) for t in sent]\n",
    "        print(f'Tok Sent {i}: {tok_sent}')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS-tagged sents - token/POS pairs\n",
    "\n",
    "if catilinam:\n",
    "    sents = L.sents(catilinam)\n",
    "    for i in range(1, 3):\n",
    "        sent = next(sents)\n",
    "        pos_sent = [f\"{t.text}/{t.pos_}\" for t in sent]\n",
    "        print(f'POS Sent {i}: {\" \".join(pos_sent)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spaCy Token objects by default\n",
    "if catilinam:\n",
    "    tokens = L.tokens(catilinam)\n",
    "    token = next(tokens)\n",
    "    print(token)\n",
    "    print(type(next(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokens as plain strings with as_text=True\n",
    "\n",
    "if catilinam:\n",
    "    plaintext_tokens = L.tokens(catilinam, as_text=True)\n",
    "    plaintext_token = next(plaintext_tokens)\n",
    "    print(plaintext_token)\n",
    "    print(type(plaintext_token))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concordance-header",
   "metadata": {},
   "source": [
    "## Concordance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a concordance: word -> list of citations where it appears\n",
    "# Note: Without Tesserae citations, uses fileid:sentN format\n",
    "\n",
    "if catullus:\n",
    "    conc = L.concordance(fileids=catullus, basis=\"lemma\")\n",
    "    print(f\"Unique lemmas: {len(conc)}\")\n",
    "    print()\n",
    "\n",
    "    # Look up a specific lemma\n",
    "    if \"amor\" in conc:\n",
    "        print(\"Citations for 'amor':\")\n",
    "        for cit in conc[\"amor\"][:10]:\n",
    "            print(f\"  {cit}\")\n",
    "        if len(conc[\"amor\"]) > 10:\n",
    "            print(f\"  ... and {len(conc['amor']) - 10} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concordance by surface text form (exact spelling)\n",
    "if catullus:\n",
    "    conc_text = L.concordance(fileids=catullus, basis=\"text\")\n",
    "\n",
    "    # Different forms of 'puella' (girl)\n",
    "    puella_forms = [\"puella\", \"puellae\", \"puellam\", \"puellas\", \"puellis\"]\n",
    "    print(\"Occurrences of 'puella' forms:\")\n",
    "    for form in puella_forms:\n",
    "        if form in conc_text:\n",
    "            count = len(conc_text[form])\n",
    "            print(f\"  {form}: {count} occurrences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kwic-header",
   "metadata": {},
   "source": [
    "## KWIC (Keyword in Context)\n",
    "\n",
    "Find words with surrounding context - useful for studying word usage patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic KWIC search - find \"amor\" with 5 tokens of context on each side\n",
    "if catullus:\n",
    "    for hit in L.kwic(\"amor\", fileids=catullus, window=5, limit=5):\n",
    "        print(f\"{hit['left']} [{hit['match']}] {hit['right']}\")\n",
    "        print(f\"  -- {hit['citation']}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KWIC by lemma - finds all forms of a word (e.g., amo, amat, amant, amavit)\n",
    "\n",
    "if catullus:\n",
    "    for hit in L.kwic(\"amo\", fileids=catullus, by_lemma=True, window=4, limit=5):\n",
    "        print(f\"{hit['left']} [{hit['match']}] {hit['right']}\")\n",
    "        print(f\"  -- {hit['citation']}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ngrams-header",
   "metadata": {},
   "source": [
    "## N-grams and Skipgrams\n",
    "\n",
    "Extract contiguous token sequences (n-grams) or sequences with gaps (skipgrams) for collocation analysis and language modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract bigrams (2-word sequences)\n",
    "from itertools import islice\n",
    "\n",
    "if catullus:\n",
    "    bigrams = list(islice(L.ngrams(n=2, fileids=catullus), 20))\n",
    "    print(\"First 20 bigrams:\")\n",
    "    pprint(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigrams (3-word sequences)\n",
    "if catullus:\n",
    "    trigrams = list(islice(L.ngrams(n=3, fileids=catullus), 10))\n",
    "    print(\"First 10 trigrams:\")\n",
    "    pprint(trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get n-grams as token tuples for linguistic analysis\n",
    "\n",
    "if catullus:\n",
    "    for gram in islice(L.ngrams(n=2, fileids=catullus, as_tuples=True), 5):\n",
    "        print([(t.text, t.lemma_, t.pos_) for t in gram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigram frequency analysis - find most common word pairs\n",
    "from collections import Counter\n",
    "\n",
    "if catullus:\n",
    "    bigram_counts = Counter(L.ngrams(n=2, fileids=catullus))\n",
    "    print(\"Most common bigrams:\")\n",
    "    for bigram, count in bigram_counts.most_common(10):\n",
    "        print(f\"  {bigram}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skipgrams - word pairs with gaps between them\n",
    "\n",
    "if catullus:\n",
    "    skipgrams = list(islice(L.skipgrams(n=2, k=1, fileids=catullus), 15))\n",
    "    print(\"First 15 skipgrams (bigrams with 1 skip):\")\n",
    "    pprint(skipgrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-grams by lemma - normalize inflected forms\n",
    "\n",
    "if catullus:\n",
    "    print(\"Bigrams by surface text:\")\n",
    "    text_bigrams = list(islice(L.ngrams(n=2, fileids=catullus, basis=\"text\"), 5))\n",
    "    pprint(text_bigrams)\n",
    "\n",
    "    print(\"\\nBigrams by lemma:\")\n",
    "    lemma_bigrams = list(islice(L.ngrams(n=2, fileids=catullus, basis=\"lemma\"), 5))\n",
    "    pprint(lemma_bigrams)\n",
    "\n",
    "    print(\"\\nMost common lemma bigrams:\")\n",
    "    lemma_counts = Counter(L.ngrams(n=2, fileids=catullus, basis=\"lemma\"))\n",
    "    for bigram, count in lemma_counts.most_common(10):\n",
    "        print(f\"  {bigram}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stats-header",
   "metadata": {},
   "source": [
    "## Basic descriptive stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick corpus overview\n",
    "files = L.fileids()\n",
    "print(f\"Total files: {len(files)}\")\n",
    "\n",
    "# Sample stats from one file\n",
    "sample_file = files[0]\n",
    "sample_text = next(L.texts(sample_file))\n",
    "print(f\"\\nSample file: {sample_file}\")\n",
    "print(f\"Character count: {len(sample_text)}\")\n",
    "print(f\"Word count (approx): {len(sample_text.split())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-58",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stats for a specific file\n",
    "\n",
    "if catullus:\n",
    "    doc = next(L.docs(catullus))\n",
    "    print(f'Stats for {catullus}:')\n",
    "    print(f'  Sentences: {len(list(doc.sents))}')\n",
    "    print(f'  Tokens: {len(doc)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "search-header",
   "metadata": {},
   "source": [
    "## Search Features\n",
    "\n",
    "All search methods from `BaseCorpusReader` are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_sents() - find sentences containing specific words\n",
    "# Works with pattern, forms, lemma, or matcher_pattern\n",
    "\n",
    "for hit in islice(L.find_sents(forms=[\"Caesar\", \"Caesarem\", \"Caesaris\"]), 5):\n",
    "    print(f\"{hit['citation']}: {hit['sentence'][:80]}...\")\n",
    "    print(f\"  Matched: {hit['matches']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_sents() by lemma - slower but finds ALL forms\n",
    "\n",
    "for hit in islice(L.find_sents(lemma=\"bellum\"), 5):\n",
    "    print(f\"{hit['citation']}: {hit['sentence'][:80]}...\")\n",
    "    print(f\"  Matched forms: {hit['matches']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_sents() with spaCy Matcher patterns\n",
    "# Search for ADJ + NOUN sequences\n",
    "\n",
    "pattern = [{\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}]\n",
    "for hit in islice(L.find_sents(matcher_pattern=pattern, fileids=L.fileids(match=\"catullus\")), 5):\n",
    "    print(f\"{hit['citation']}: {hit['sentence'][:80]}...\")\n",
    "    print(f\"  Matched: {hit['matches']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More complex Matcher patterns\n",
    "# Find sentences with a specific lemma followed by a noun\n",
    "\n",
    "pattern = [{\"LEMMA\": \"magnus\"}, {\"POS\": \"NOUN\"}]\n",
    "for hit in islice(L.find_sents(matcher_pattern=pattern), 5):\n",
    "    print(f\"{hit['citation']}: {hit['matches']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "levels-header",
   "metadata": {},
   "source": [
    "## Annotation Levels\n",
    "\n",
    "Control NLP processing overhead with `AnnotationLevel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AnnotationLevel controls how much NLP processing to apply\n",
    "\n",
    "# NONE - use texts() for raw strings (fastest)\n",
    "# TOKENIZE - tokenization + sentence boundaries only\n",
    "# BASIC - adds lemmatization and POS tagging (default)\n",
    "# FULL - full pipeline including NER and dependency parsing\n",
    "\n",
    "reader_fast = LatinLibraryReader(annotation_level=AnnotationLevel.TOKENIZE)\n",
    "reader_full = LatinLibraryReader(annotation_level=AnnotationLevel.FULL)\n",
    "\n",
    "print(\"Available annotation levels:\")\n",
    "for level in AnnotationLevel:\n",
    "    print(f\"  {level.name}: {level.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caching-header",
   "metadata": {},
   "source": [
    "## Document Caching\n",
    "\n",
    "Documents are cached by default for better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-cache",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check cache statistics\n",
    "print(\"Cache stats:\", L.cache_stats())\n",
    "\n",
    "# Clear the cache if needed\n",
    "# L.clear_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selector-header",
   "metadata": {},
   "source": [
    "## FileSelector API\n",
    "\n",
    "Fluent file filtering with `select()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-selector-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by filename pattern\n",
    "selection = L.select().match(r\"vergil\")\n",
    "print(f\"Vergil files: {len(selection)}\")\n",
    "print(selection.preview(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-selector-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain multiple filters\n",
    "selection = L.select().match(r\"cicero\")\n",
    "print(f\"Cicero files: {len(selection)}\")\n",
    "\n",
    "# Use with docs()\n",
    "for doc in islice(L.docs(selection), 2):\n",
    "    print(f\"{doc._.fileid}: {len(list(doc.sents))} sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison-header",
   "metadata": {},
   "source": [
    "## Comparison with TesseraeReader\n",
    "\n",
    "| Feature | TesseraeReader | LatinLibraryReader |\n",
    "|---------|----------------|--------------------|\n",
    "| File format | `.tess` | `.txt` |\n",
    "| Citation system | Built-in (`<author. work. line>`) | Fallback (`fileid:sentN`) |\n",
    "| `lines()` | Yes (citation-annotated spans) | No |\n",
    "| `doc_rows()` | Yes (citation -> span mapping) | No |\n",
    "| `paras()` | No (format doesn't support) | Yes |\n",
    "| `texts_by_line()` | Yes | No |\n",
    "| `search()` | Yes (fast regex on lines) | No |\n",
    "| `find_lines()` | Yes | No |\n",
    "| `find_sents()` | Yes | Yes |\n",
    "| Metadata | Rich (author, date, genre) | Minimal (title from first line) |\n",
    "| Auto-download | Yes | Yes |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
