{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "\n",
    "from latincyreaders import TesseraeReader, AnnotationLevel\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up reader\n",
    "\n",
    "T = TesseraeReader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fileids and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First 10 filenames\n",
    "\n",
    "files = T.fileids()[:10]\n",
    "pprint(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get files by pattern match (regex)\n",
    "files = T.fileids(match='horace')\n",
    "pprint(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get files by pattern - supports regex\n",
    "files = T.fileids(match=r'vergil.*aeneid')\n",
    "pprint(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get files by partial match\n",
    "files = T.fileids(match='cicero')[:10]\n",
    "pprint(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case-insensitive regex matching\n",
    "files = T.fileids(match=r'ovid')\n",
    "pprint(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple pattern examples\n",
    "files = T.fileids(match='lucretius')\n",
    "pprint(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all files\n",
    "all_files = T.fileids()\n",
    "print(f\"Total files: {len(all_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering by metadata\n",
    "\n",
    "The `match` parameter uses regex, which can be too broad. For precise filtering, use metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem: regex match can be too broad\n",
    "# Searching for \"lucretius\" also finds \"antilucretius\"\n",
    "\n",
    "files = T.fileids(match='lucretius')\n",
    "pprint(files)\n",
    "# Note: polignac.antilucretius is included!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: filter by exact author using metadata\n",
    "# This gets ONLY Lucretius, not Anti-Lucretius\n",
    "\n",
    "lucretius_files = [\n",
    "    fileid for fileid, meta in T.metadata()\n",
    "    if meta.get('author') == 'Lucretius'\n",
    "]\n",
    "pprint(lucretius_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by genre (e.g., find all epic poetry)\n",
    "\n",
    "epic_files = [\n",
    "    fileid for fileid, meta in T.metadata()\n",
    "    if meta.get('genre') == 'epic'\n",
    "]\n",
    "print(f\"Epic texts: {len(epic_files)} files\")\n",
    "pprint(epic_files[:10])  # First 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by date: texts before 50 BCE (negative dates = BCE)\n",
    "\n",
    "def get_date(meta):\n",
    "    \"\"\"Parse date string to int, handling missing/invalid values.\"\"\"\n",
    "    try:\n",
    "        return int(meta.get('date', 0))\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "\n",
    "# Texts from before 50 BCE\n",
    "early_republic = [\n",
    "    fileid for fileid, meta in T.metadata()\n",
    "    if (d := get_date(meta)) is not None and d < -50\n",
    "]\n",
    "print(f\"Texts before 50 BCE: {len(early_republic)} files\")\n",
    "pprint(early_republic[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augustan era texts (roughly 43 BCE - 14 CE)\n",
    "\n",
    "augustan = [\n",
    "    fileid for fileid, meta in T.metadata()\n",
    "    if (d := get_date(meta)) is not None and -43 <= d <= 14\n",
    "]\n",
    "print(f\"Augustan era texts: {len(augustan)} files\")\n",
    "pprint(augustan[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See all available genres in the corpus\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "genres = Counter(\n",
    "    meta.get('genre') for _, meta in T.metadata()\n",
    "    if meta.get('genre')\n",
    ")\n",
    "print(\"Available genres:\")\n",
    "for genre, count in genres.most_common():\n",
    "    print(f\"  {genre}: {count} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine filters: lyric poetry from the Augustan era\n",
    "\n",
    "augustan_lyric = [\n",
    "    fileid for fileid, meta in T.metadata()\n",
    "    if meta.get('genre') == 'lyric'\n",
    "    and (d := get_date(meta)) is not None \n",
    "    and -43 <= d <= 14\n",
    "]\n",
    "print(f\"Augustan lyric poetry: {len(augustan_lyric)} files\")\n",
    "pprint(augustan_lyric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata can be accessed without NLP processing (instant)\n",
    "# Use get_metadata() to avoid loading the spaCy model\n",
    "\n",
    "catullus = 'catullus.carmina.tess'\n",
    "\n",
    "# Fast: get metadata directly (no NLP overhead)\n",
    "meta = T.get_metadata(catullus)\n",
    "\n",
    "print(f\"Metadata for {catullus}:\")\n",
    "pprint(meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a file to work with\n",
    "catullus = 'catullus.carmina.tess'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Docs - spaCy Doc objects with NLP annotations\n",
    "\n",
    "catullus_doc = next(T.docs(catullus))\n",
    "print(catullus_doc[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Texts - raw strings (zero NLP overhead)\n",
    "\n",
    "catullus_text = next(T.texts(catullus))\n",
    "pprint(catullus_text[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Doc Rows - citation -> text mapping\n",
    "\n",
    "catullus_docrows = next(T.doc_rows(catullus))\n",
    "\n",
    "print('First 10 citation -> span mappings:')\n",
    "for i, (citation, span) in enumerate(catullus_docrows.items()):\n",
    "    if i >= 10:\n",
    "        break\n",
    "    print(f\"  {citation}: {span.text[:40]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another file for examples\n",
    "catilinam = 'cicero.in_catilinam.tess'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Paras (not implemented - Tesserae format doesn't have paragraphs)\n",
    "# Use sents() or lines() instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sents - spaCy Span objects\n",
    "\n",
    "# Segmentation and tokenization done using la_core_web_lg model\n",
    "catilinam_sents = T.sents(catilinam)\n",
    "\n",
    "for i in range(1, 6):\n",
    "    print(f'Sent {i}: {next(catilinam_sents)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokens - spaCy Token objects\n",
    "\n",
    "catilinam_tokens = T.tokens(catilinam)\n",
    "\n",
    "for i in range(1, 10):\n",
    "    print(f'Word {i}: {next(catilinam_tokens)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spaCy Token has many useful attributes\n",
    "catilinam_tokens = T.tokens(catilinam)\n",
    "token = next(catilinam_tokens)\n",
    "print(f\"Available attributes: {[a for a in dir(token) if not a.startswith('_')][:15]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token linguistic attributes (BASIC level: text, lemma, POS, tag)\n",
    "# Note: dep_ requires AnnotationLevel.FULL\n",
    "\n",
    "catilinam_tokens = T.tokens(catilinam)\n",
    "t = next(catilinam_tokens)\n",
    "print(f\"text: {t.text}, lemma: {t.lemma_}, pos: {t.pos_}, tag: {t.tag_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For custom text processing, work with the raw text or spaCy Doc\n",
    "# The preprocess parameter has been removed - use spaCy pipeline components instead\n",
    "\n",
    "# Get text as strings\n",
    "for token_text in T.tokens(catilinam, as_text=True):\n",
    "    # Apply your own processing\n",
    "    processed = token_text.lower()\n",
    "    print(processed, end=' ')\n",
    "    break  # Just show first token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenized sents - use spaCy directly\n",
    "# Get (token, lemma, tag) tuples from sentences\n",
    "\n",
    "catilinam_sents = T.sents(catilinam)\n",
    "\n",
    "for i in range(1, 4):\n",
    "    sent = next(catilinam_sents)\n",
    "    tok_sent = [(t.text, t.lemma_, t.tag_) for t in sent]\n",
    "    print(f'Tok Sent {i}: {tok_sent}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenized sents, simplified (just tokens as strings)\n",
    "\n",
    "catilinam_sents = T.sents(catilinam)\n",
    "\n",
    "for i in range(1, 4):\n",
    "    sent = next(catilinam_sents)\n",
    "    tok_sent = [t.text for t in sent]\n",
    "    print(f'Tok Sent {i}: {tok_sent}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS-tagged sents - token/POS pairs\n",
    "\n",
    "catilinam_sents = T.sents(catilinam)\n",
    "\n",
    "for i in range(1, 3):\n",
    "    sent = next(catilinam_sents)\n",
    "    pos_sent = [f\"{t.text}/{t.pos_}\" for t in sent]\n",
    "    print(f'POS Sent {i}: {\" \".join(pos_sent)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spaCy Token objects by default\n",
    "catilinam_tokens = T.tokens(catilinam)\n",
    "\n",
    "catilinam_token = next(catilinam_tokens)\n",
    "print(catilinam_token)\n",
    "print(type(next(catilinam_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokens as plain strings with as_text=True\n",
    "\n",
    "plaintext_tokens = T.tokens(catilinam, as_text=True)\n",
    "\n",
    "plaintext_token = next(plaintext_tokens)\n",
    "print(plaintext_token)\n",
    "print(type(plaintext_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lines (citation units from the Tesserae format)\n",
    "\n",
    "aeneid = T.fileids(match='aeneid')[0]\n",
    "\n",
    "aeneid_lines = T.lines(aeneid)\n",
    "\n",
    "for i in range(1, 9):\n",
    "    print(f'{i}: {next(aeneid_lines)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lines with citation information preserved\n",
    "\n",
    "aeneid_lines = T.lines(aeneid)\n",
    "\n",
    "for i in range(1, 9):\n",
    "    line = next(aeneid_lines)\n",
    "    print(f'{line._.citation}: {line}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentences with citation information\n",
    "# Sentences can span multiple citation lines\n",
    "\n",
    "aeneid_doc = next(T.docs(aeneid))\n",
    "\n",
    "# Show first few sentences with their citation ranges\n",
    "for i, sent in enumerate(aeneid_doc.sents):\n",
    "    if i >= 3:\n",
    "        break\n",
    "    overlapping = [\n",
    "        span._.citation for span in aeneid_doc.spans.get(\"lines\", [])\n",
    "        if sent.start < span.end and sent.end > span.start\n",
    "    ]\n",
    "    if overlapping:\n",
    "        cit_range = f\"{overlapping[0]}–{overlapping[-1]}\" if len(overlapping) > 1 else overlapping[0]\n",
    "    else:\n",
    "        cit_range = \"?\"\n",
    "    print(f\"{cit_range}\")\n",
    "    print(f\"  {sent.text[:80]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokens within citation lines\n",
    "# Access citation via the line spans\n",
    "\n",
    "aeneid_doc = next(T.docs(aeneid))\n",
    "line = aeneid_doc.spans[\"lines\"][0]\n",
    "\n",
    "print(f\"Line citation: {line._.citation}\")\n",
    "print(f\"Tokens in line: {[t.text for t in line]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get files by pattern\n",
    "metamorphoses = T.fileids(match='ovid.metamorphoses')\n",
    "pprint(metamorphoses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom text normalization example\n",
    "# Use this pattern when you need specific preprocessing\n",
    "\n",
    "def normalize_latin(text):\n",
    "    \"\"\"Normalize Latin text for analysis.\"\"\"\n",
    "    text = text.lower()\n",
    "    # Normalize u/v and i/j\n",
    "    text = text.replace('v', 'u').replace('j', 'i')\n",
    "    # Remove punctuation  \n",
    "    import string\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return \" \".join(text.split()).strip()\n",
    "\n",
    "# Example usage on raw text\n",
    "sample = next(T.texts(metamorphoses[0]))[:100]\n",
    "print(f\"Original: {sample}\")\n",
    "print(f\"Normalized: {normalize_latin(sample)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Concordance\n",
    "\n",
    "# Build a concordance: word -> list of citations where it appears\n",
    "# Group by lemma (default) to see all forms of a word together\n",
    "\n",
    "catullus_conc = T.concordance(fileids=catullus, basis=\"lemma\")\n",
    "\n",
    "print(f\"Unique lemmas in Catullus: {len(catullus_conc)}\")\n",
    "print()\n",
    "\n",
    "# Look up a specific lemma\n",
    "if \"amor\" in catullus_conc:\n",
    "    print(\"Citations for 'amor':\")\n",
    "    for cit in catullus_conc[\"amor\"][:10]:\n",
    "        print(f\"  {cit}\")\n",
    "    if len(catullus_conc[\"amor\"]) > 10:\n",
    "        print(f\"  ... and {len(catullus_conc['amor']) - 10} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concordance by surface text form (exact spelling)\n",
    "catullus_conc_text = T.concordance(fileids=catullus, basis=\"text\")\n",
    "\n",
    "# Different forms of 'puella' (girl)\n",
    "puella_forms = [\"puella\", \"puellae\", \"puellam\", \"puellas\", \"puellis\"]\n",
    "print(\"Occurrences of 'puella' forms in Catullus:\")\n",
    "for form in puella_forms:\n",
    "    if form in catullus_conc_text:\n",
    "        count = len(catullus_conc_text[form])\n",
    "        print(f\"  {form}: {count} occurrences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KWIC (Keyword in Context)\n",
    "\n",
    "Find words with surrounding context - useful for studying word usage patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic KWIC search - find \"amor\" with 5 tokens of context on each side\n",
    "for hit in T.kwic(\"amor\", fileids=catullus, window=5, limit=5):\n",
    "    print(f\"{hit['left']} [{hit['match']}] {hit['right']}\")\n",
    "    print(f\"  -- {hit['citation']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KWIC by lemma - finds all forms of a word (e.g., amo, amat, amant, amavit)\n",
    "# Use by_lemma=True to match against lemmatized forms\n",
    "\n",
    "for hit in T.kwic(\"amo\", fileids=catullus, by_lemma=True, window=4, limit=5):\n",
    "    print(f\"{hit['left']} [{hit['match']}] {hit['right']}\")\n",
    "    print(f\"  -- {hit['citation']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams and Skipgrams\n",
    "\n",
    "Extract contiguous token sequences (n-grams) or sequences with gaps (skipgrams) for collocation analysis and language modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract bigrams (2-word sequences) from Catullus\n",
    "# By default, returns strings and filters out punctuation\n",
    "\n",
    "from itertools import islice\n",
    "\n",
    "bigrams = list(islice(T.ngrams(n=2, fileids=catullus), 20))\n",
    "print(\"First 20 bigrams from Catullus:\")\n",
    "pprint(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigrams (3-word sequences)\n",
    "trigrams = list(islice(T.ngrams(n=3, fileids=catullus), 10))\n",
    "print(\"First 10 trigrams:\")\n",
    "pprint(trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get n-grams as token tuples for linguistic analysis\n",
    "# as_tuples=True returns tuples of spaCy Token objects\n",
    "\n",
    "for gram in islice(T.ngrams(n=2, fileids=catullus, as_tuples=True), 5):\n",
    "    # Access token attributes: text, lemma, POS\n",
    "    print([(t.text, t.lemma_, t.pos_) for t in gram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigram frequency analysis - find most common word pairs\n",
    "from collections import Counter\n",
    "\n",
    "bigram_counts = Counter(T.ngrams(n=2, fileids=catullus))\n",
    "print(\"Most common bigrams in Catullus:\")\n",
    "for bigram, count in bigram_counts.most_common(10):\n",
    "    print(f\"  {bigram}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skipgrams - word pairs with gaps between them\n",
    "# n=2 (pairs), k=1 (allow 1 word gap)\n",
    "# \"the quick brown fox\" → \"the quick\", \"the brown\", \"quick brown\", \"quick fox\", ...\n",
    "\n",
    "skipgrams = list(islice(T.skipgrams(n=2, k=1, fileids=catullus), 15))\n",
    "print(\"First 15 skipgrams (bigrams with 1 skip):\")\n",
    "pprint(skipgrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-grams by lemma - normalize inflected forms to dictionary headwords\n",
    "# Useful for finding collocations regardless of grammatical case/number\n",
    "\n",
    "# Compare: text basis (default) vs lemma basis\n",
    "print(\"Bigrams by surface text:\")\n",
    "text_bigrams = list(islice(T.ngrams(n=2, fileids=catullus, basis=\"text\"), 5))\n",
    "pprint(text_bigrams)\n",
    "\n",
    "print(\"\\nBigrams by lemma (normalized forms):\")\n",
    "lemma_bigrams = list(islice(T.ngrams(n=2, fileids=catullus, basis=\"lemma\"), 5))\n",
    "pprint(lemma_bigrams)\n",
    "\n",
    "# Lemma-based frequency counts group inflected variants together\n",
    "print(\"\\nMost common lemma bigrams:\")\n",
    "lemma_counts = Counter(T.ngrams(n=2, fileids=catullus, basis=\"lemma\"))\n",
    "for bigram, count in lemma_counts.most_common(10):\n",
    "    print(f\"  {bigram}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Basic descriptive stats\n",
    "# Count files, estimate tokens, etc.\n",
    "\n",
    "# Quick corpus overview\n",
    "files = T.fileids()\n",
    "print(f\"Total files: {len(files)}\")\n",
    "\n",
    "# Sample stats from one file\n",
    "sample_file = files[0]\n",
    "sample_text = next(T.texts(sample_file))\n",
    "print(f\"\\nSample file: {sample_file}\")\n",
    "print(f\"Character count: {len(sample_text)}\")\n",
    "print(f\"Word count (approx): {len(sample_text.split())}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample output for full corpus\n",
    "\n",
    "A full describe() method will be added in a future release."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stats for a specific file\n",
    "\n",
    "catullus_doc = next(T.docs(catullus))\n",
    "\n",
    "print(f'Stats for {catullus}:')\n",
    "print(f'  Sentences: {len(list(catullus_doc.sents))}')\n",
    "print(f'  Tokens: {len(catullus_doc)}')\n",
    "print(f'  Citation lines: {len(catullus_doc.spans.get(\"lines\", []))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Features in latincyreaders\n",
    "\n",
    "The following sections demonstrate new search and filtering capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search() - fast regex search across the corpus (no NLP required)\n",
    "from itertools import islice\n",
    "\n",
    "# Find lines mentioning Thebes (limit to first 5 results)\n",
    "results = T.search(r'\\bTheb\\w+\\b')\n",
    "for fileid, citation, text, matches in islice(results, 5):\n",
    "    print(f\"{fileid} {citation}: found {matches}\")\n",
    "    print(f\"  → {text[:60]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_lines() - find citation lines containing specific words/patterns\n",
    "\n",
    "# Find lines with specific word forms\n",
    "forms = [\"Thebas\", \"Thebarum\", \"Thebis\"]\n",
    "for fileid, citation, text in islice(T.find_lines(forms=forms), 5):\n",
    "    print(f\"{citation}: {text[:70]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_sents() - find sentences containing specific words\n",
    "# Fast path: search by exact forms (uses regex, minimal NLP)\n",
    "\n",
    "for hit in islice(T.find_sents(forms=[\"Caesar\", \"Caesarem\", \"Caesaris\"]), 5):\n",
    "    print(f\"{hit['citation']}: {hit['sentence'][:80]}...\")\n",
    "    print(f\"  Matched: {hit['matches']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_sents() by lemma - slower but finds ALL forms\n",
    "# Uses NLP to lemmatize, so it catches forms you might miss\n",
    "\n",
    "# Find all sentences with any form of \"bellum\" (war)\n",
    "for hit in islice(T.find_sents(lemma=\"bellum\"), 5):\n",
    "    print(f\"{hit['citation']}: {hit['sentence'][:80]}...\")\n",
    "    print(f\"  Matched forms: {hit['matches']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_sents() with spaCy Matcher patterns - advanced pattern matching\n",
    "# Search for ADJ + NOUN sequences (e.g., \"magna voce\", \"pulchra puella\")\n",
    "\n",
    "pattern = [{\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}]\n",
    "for hit in islice(T.find_sents(matcher_pattern=pattern, fileids=T.fileids(match=\"catullus\")), 5):\n",
    "    print(f\"{hit['citation']}: {hit['sentence'][:80]}...\")\n",
    "    print(f\"  Matched: {hit['matches']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More complex Matcher patterns\n",
    "# Find sentences with a specific lemma followed by a noun\n",
    "\n",
    "pattern = [{\"LEMMA\": \"magnus\"}, {\"POS\": \"NOUN\"}]\n",
    "for hit in islice(T.find_sents(matcher_pattern=pattern), 5):\n",
    "    print(f\"{hit['citation']}: {hit['matches']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotation Levels\n",
    "\n",
    "Control NLP processing overhead with `AnnotationLevel`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AnnotationLevel controls how much NLP processing to apply\n",
    "\n",
    "# NONE - use texts() for raw strings (fastest)\n",
    "# TOKENIZE - tokenization + sentence boundaries only\n",
    "# BASIC - adds lemmatization and POS tagging (default)\n",
    "# FULL - full pipeline including NER and dependency parsing\n",
    "\n",
    "# Create readers with different annotation levels\n",
    "reader_fast = TesseraeReader(annotation_level=AnnotationLevel.TOKENIZE)\n",
    "reader_full = TesseraeReader(annotation_level=AnnotationLevel.FULL)\n",
    "\n",
    "print(\"Available annotation levels:\")\n",
    "for level in AnnotationLevel:\n",
    "    print(f\"  {level.name}: {level.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export search results to TSV, CSV, or JSONL\n",
    "\n",
    "results = T.find_sents(forms=[\"amor\", \"amoris\", \"amorem\"], fileids=T.fileids(match=\"catullus\"))\n",
    "export = T.export_search_results(results, format=\"tsv\")\n",
    "\n",
    "print(\"TSV export (first 500 chars):\")\n",
    "print(export[:500])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
