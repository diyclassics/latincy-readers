{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "\n",
    "from latincyreaders import GreekTesseraeReader, AnnotationLevel\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up reader\n",
    "\n",
    "# TOKENIZE level: fast, no OdyCy model needed\n",
    "G = GreekTesseraeReader(annotation_level=AnnotationLevel.TOKENIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fileids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First 10 filenames\n",
    "\n",
    "files = G.fileids()[:10]\n",
    "pprint(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get files by pattern match (regex)\n",
    "files = G.fileids(match='homer')\n",
    "pprint(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Homer's Iliad files\n",
    "iliad_files = G.fileids(match=r'homer.*iliad')\n",
    "pprint(iliad_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Homer's Odyssey files\n",
    "odyssey_files = G.fileids(match=r'homer.*odyssey')\n",
    "pprint(odyssey_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other authors\n",
    "files = G.fileids(match='sophocles')\n",
    "pprint(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = G.fileids(match='euripides')\n",
    "pprint(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all files\n",
    "all_files = G.fileids()\n",
    "print(f\"Total files: {len(all_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Texts by line (zero NLP overhead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts_by_line() - fastest way to read Tesserae files\n",
    "# Returns (citation, text) pairs with zero NLP processing\n",
    "\n",
    "iliad_1 = iliad_files[0] if iliad_files else G.fileids(match='homer')[0]\n",
    "\n",
    "for i, (citation, text) in enumerate(G.texts_by_line(iliad_1)):\n",
    "    if i >= 10:\n",
    "        break\n",
    "    print(f\"{citation}: {text[:70]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw text (entire document as one string)\n",
    "iliad_1_text = next(G.texts(iliad_1))\n",
    "print(f\"Character count: {len(iliad_1_text)}\")\n",
    "print(f\"Word count (approx): {len(iliad_1_text.split())}\")\n",
    "print(f\"\\nFirst 200 chars: {iliad_1_text[:200]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Docs - spaCy Doc objects\n",
    "\n",
    "iliad_doc = next(G.docs(iliad_1))\n",
    "print(iliad_doc[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Doc Rows - citation -> text mapping\n",
    "\n",
    "iliad_docrows = next(G.doc_rows(iliad_1))\n",
    "\n",
    "print('First 10 citation -> span mappings:')\n",
    "for i, (citation, span) in enumerate(iliad_docrows.items()):\n",
    "    if i >= 10:\n",
    "        break\n",
    "    print(f\"  {citation}: {span.text[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sents - spaCy Span objects\n",
    "\n",
    "iliad_sents = G.sents(iliad_1)\n",
    "\n",
    "for i in range(1, 6):\n",
    "    print(f'Sent {i}: {next(iliad_sents)}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokens\n",
    "\n",
    "iliad_tokens = G.tokens(iliad_1)\n",
    "\n",
    "for i in range(1, 15):\n",
    "    print(f'Word {i}: {next(iliad_tokens)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokens as plain strings\n",
    "\n",
    "plaintext_tokens = G.tokens(iliad_1, as_text=True)\n",
    "\n",
    "for i in range(1, 10):\n",
    "    t = next(plaintext_tokens)\n",
    "    print(f'{t} ({type(t).__name__})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lines (citation units from the Tesserae format)\n",
    "\n",
    "iliad_lines = G.lines(iliad_1)\n",
    "\n",
    "for i in range(1, 9):\n",
    "    line = next(iliad_lines)\n",
    "    print(f'{line._.citation}: {line}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doc stats\n",
    "\n",
    "iliad_doc = next(G.docs(iliad_1))\n",
    "\n",
    "print(f'Stats for {iliad_1}:')\n",
    "print(f'  Sentences: {len(list(iliad_doc.sents))}')\n",
    "print(f'  Tokens: {len(iliad_doc)}')\n",
    "print(f'  Citation lines: {len(iliad_doc.spans.get(\"lines\", []))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search\n",
    "\n",
    "Fast regex-based search across the corpus. No NLP model required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search() - find lines matching a regex\n",
    "# Search for Achilles across all Homer\n",
    "\n",
    "homer_files = G.fileids(match='homer')\n",
    "\n",
    "results = G.search(r'Ἀχιλ', fileids=homer_files)\n",
    "for fileid, citation, text, matches in islice(results, 10):\n",
    "    print(f\"{citation}: found {matches}\")\n",
    "    print(f\"  → {text[:70]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_lines() - find citation lines with specific words/patterns\n",
    "\n",
    "# Search for Zeus across corpus\n",
    "for fileid, citation, text in islice(G.find_lines(pattern=r'Ζεὺς'), 10):\n",
    "    print(f\"{citation}: {text[:70]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_lines() with specific forms\n",
    "\n",
    "forms = [\"μῆνιν\", \"μῆνις\", \"μήνιος\", \"μήνιδος\"]\n",
    "for fileid, citation, text in G.find_lines(forms=forms, fileids=homer_files):\n",
    "    print(f\"{citation}: {text[:70]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_sents() - find sentences containing a pattern\n",
    "\n",
    "for hit in islice(G.find_sents(pattern=r'Ἀχιλ', fileids=homer_files), 5):\n",
    "    print(f\"{hit['citation']}: {hit['sentence'][:80]}...\")\n",
    "    print(f\"  Matched: {hit['matches']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_sents() with context\n",
    "\n",
    "for hit in islice(G.find_sents(pattern=r'μῆνιν', fileids=homer_files, context=True), 3):\n",
    "    if hit.get('prev_sent'):\n",
    "        print(f\"  [prev] {hit['prev_sent'][:60]}...\")\n",
    "    print(f\"  >>> {hit['sentence'][:80]}...\")\n",
    "    if hit.get('next_sent'):\n",
    "        print(f\"  [next] {hit['next_sent'][:60]}...\")\n",
    "    print(f\"  -- {hit['citation']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KWIC (Keyword in Context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic KWIC search\n",
    "for hit in G.kwic(\"θεὰ\", fileids=homer_files, window=5, limit=10):\n",
    "    print(f\"{hit['left']:>50s} [{hit['match']}] {hit['right']:<50s}\")\n",
    "    print(f\"{'':>50s}  {hit['citation']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KWIC for a character name\n",
    "for hit in G.kwic(\"Ἕκτωρ\", fileids=homer_files, window=5, limit=10):\n",
    "    print(f\"{hit['left']:>50s} [{hit['match']}] {hit['right']:<50s}\")\n",
    "    print(f\"{'':>50s}  {hit['citation']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigrams from Iliad Book 1\n",
    "\n",
    "bigrams = list(islice(G.ngrams(n=2, fileids=iliad_1), 20))\n",
    "print(\"First 20 bigrams from Iliad 1:\")\n",
    "pprint(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigrams\n",
    "trigrams = list(islice(G.ngrams(n=3, fileids=iliad_1), 10))\n",
    "print(\"First 10 trigrams:\")\n",
    "pprint(trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigram frequency analysis\n",
    "from collections import Counter\n",
    "\n",
    "bigram_counts = Counter(G.ngrams(n=2, fileids=homer_files))\n",
    "print(\"Most common bigrams in Homer:\")\n",
    "for bigram, count in bigram_counts.most_common(15):\n",
    "    print(f\"  {bigram}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skipgrams - word pairs with gaps\n",
    "# n=2 (pairs), k=1 (allow 1 word gap)\n",
    "\n",
    "skipgrams = list(islice(G.skipgrams(n=2, k=1, fileids=iliad_1), 15))\n",
    "print(\"First 15 skipgrams (bigrams with 1 skip):\")\n",
    "pprint(skipgrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple word counting from raw text (fastest method)\n",
    "\n",
    "word_counts = Counter()\n",
    "\n",
    "for citation, text in G.texts_by_line(fileids=homer_files):\n",
    "    words = text.split()\n",
    "    word_counts.update(words)\n",
    "\n",
    "print(f\"Total word tokens: {sum(word_counts.values())}\")\n",
    "print(f\"Unique word types: {len(word_counts)}\")\n",
    "print(\"\\nMost common words in Homer:\")\n",
    "for word, count in word_counts.most_common(20):\n",
    "    print(f\"  {word:20s} {count:>6d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character mention counts\n",
    "\n",
    "characters = {\n",
    "    \"Achilles\": r\"Ἀχιλ\",\n",
    "    \"Hector\": r\"Ἕκτ\",\n",
    "    \"Odysseus\": r\"Ὀδυσ\",\n",
    "    \"Zeus\": r\"Ζε[υύ]\",\n",
    "    \"Athena\": r\"Ἀθην\",\n",
    "    \"Apollo\": r\"Ἀπόλλ\",\n",
    "    \"Agamemnon\": r\"Ἀγαμέμν\",\n",
    "    \"Patroclus\": r\"Πατρόκλ\",\n",
    "}\n",
    "\n",
    "print(\"Character mentions (lines) in Homer:\")\n",
    "for name, pattern in characters.items():\n",
    "    count = len(list(G.search(pattern, fileids=homer_files)))\n",
    "    print(f\"  {name:15s} {count:>4d} lines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export search results to TSV\n",
    "\n",
    "results = G.find_sents(pattern=r'Ἀχιλ', fileids=homer_files)\n",
    "export = G.export_search_results(results, format=\"tsv\")\n",
    "\n",
    "print(\"TSV export (first 500 chars):\")\n",
    "print(export[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export as JSONL\n",
    "\n",
    "results = G.find_sents(forms=[\"μῆνιν\"], fileids=homer_files)\n",
    "export = G.export_search_results(results, format=\"jsonl\")\n",
    "print(export)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## With OdyCy NLP model (BASIC/FULL annotation)\n\nThe following cells require the OdyCy model for lemmatization and POS tagging.\n\n```bash\npip install https://huggingface.co/chcaa/grc_odycy_joint_sm/resolve/main/grc_odycy_joint_sm-any-py3-none-any.whl\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload with BASIC annotation level for lemmatization + POS\n",
    "G_nlp = GreekTesseraeReader(annotation_level=AnnotationLevel.BASIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS-tagged sentences\n",
    "\n",
    "iliad_sents = G_nlp.sents(iliad_1)\n",
    "\n",
    "for i in range(1, 4):\n",
    "    sent = next(iliad_sents)\n",
    "    pos_sent = [f\"{t.text}/{t.pos_}\" for t in sent]\n",
    "    print(f'POS Sent {i}: {\" \".join(pos_sent)}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token linguistic attributes\n",
    "\n",
    "iliad_sents = G_nlp.sents(iliad_1)\n",
    "sent = next(iliad_sents)\n",
    "\n",
    "for t in sent:\n",
    "    print(f\"  {t.text:20s} lemma={t.lemma_:20s} pos={t.pos_:8s} tag={t.tag_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concordance by lemma\n",
    "\n",
    "homer_files = G_nlp.fileids(match='homer')\n",
    "conc = G_nlp.concordance(fileids=homer_files[:2], basis=\"lemma\")\n",
    "\n",
    "print(f\"Unique lemmas: {len(conc)}\")\n",
    "print(\"\\nMost cited lemmas:\")\n",
    "top_lemmas = sorted(conc.items(), key=lambda x: len(x[1]), reverse=True)[:15]\n",
    "for lemma, citations in top_lemmas:\n",
    "    print(f\"  {lemma:20s} {len(citations):>4d} occurrences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KWIC by lemma - matches all inflected forms\n",
    "\n",
    "for hit in G_nlp.kwic(\"θεός\", fileids=homer_files[:2], by_lemma=True, window=4, limit=5):\n",
    "    print(f\"{hit['left']:>40s} [{hit['match']}] {hit['right']:<40s}\")\n",
    "    print(f\"{'':>40s}  {hit['citation']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-grams by lemma\n",
    "\n",
    "print(\"Most common lemma bigrams in Homer (sample):\")\n",
    "lemma_counts = Counter(G_nlp.ngrams(n=2, fileids=homer_files[:2], basis=\"lemma\"))\n",
    "for bigram, count in lemma_counts.most_common(10):\n",
    "    print(f\"  {bigram}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_sents() by lemma - finds ALL inflected forms\n",
    "\n",
    "for hit in islice(G_nlp.find_sents(lemma=\"θεός\", fileids=homer_files[:1]), 5):\n",
    "    print(f\"{hit['citation']}: {hit['sentence'][:80]}...\")\n",
    "    print(f\"  Matched forms: {hit['matches']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotation levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AnnotationLevel controls NLP processing overhead\n",
    "# NONE     - texts() only (fastest, no spaCy)\n",
    "# TOKENIZE - tokenization + sentence boundaries (spacy.blank('grc'))\n",
    "# BASIC    - + lemmatization, POS tagging (OdyCy, disable parser/NER)\n",
    "# FULL     - full pipeline including NER and deps (OdyCy)\n",
    "\n",
    "print(\"Available annotation levels:\")\n",
    "for level in AnnotationLevel:\n",
    "    print(f\"  {level.name}: {level.value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}