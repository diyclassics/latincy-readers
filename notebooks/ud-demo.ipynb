{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Universal Dependencies Reader Demo\n",
    "\n",
    "This notebook demonstrates the `UDReader` and `LatinUDReader` for working with Latin Universal Dependencies treebanks.\n",
    "\n",
    "**Key features:**\n",
    "- Parses CoNLL-U format files\n",
    "- Constructs spaCy Docs directly from gold-standard UD annotations\n",
    "- Preserves all UD data in `token._.ud` extension\n",
    "- Sentence spans with `sent_id` as citations\n",
    "- Auto-download for 6 Latin treebanks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "\n",
    "from latincyreaders import UDReader, LatinUDReader, PROIELReader\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treebanks-header",
   "metadata": {},
   "source": [
    "## Available Latin Treebanks\n",
    "\n",
    "There are 6 Latin UD treebanks available for auto-download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See all available Latin UD treebanks\n",
    "\n",
    "treebanks = LatinUDReader.available_treebanks()\n",
    "print(\"Available Latin UD Treebanks:\")\n",
    "print()\n",
    "for name, description in treebanks.items():\n",
    "    print(f\"  {name:10} - {description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a specific treebank (PROIEL - contains Caesar, Cicero, Vulgate)\n",
    "# This will prompt for confirmation if not already downloaded\n",
    "\n",
    "reader = PROIELReader()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "files-header",
   "metadata": {},
   "source": [
    "## File Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available files\n",
    "\n",
    "files = reader.fileids()\n",
    "print(f\"Total files: {len(files)}\")\n",
    "print()\n",
    "pprint(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by pattern (regex)\n",
    "\n",
    "train_files = reader.fileids(match='train')\n",
    "print(\"Training files:\")\n",
    "pprint(train_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "docs-header",
   "metadata": {},
   "source": [
    "## Working with Documents\n",
    "\n",
    "Unlike other readers, `UDReader` constructs spaCy Docs directly from the gold-standard UD annotations. It does **not** run the spaCy NLP pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first document\n",
    "\n",
    "doc = next(reader.docs())\n",
    "\n",
    "print(f\"Fileid: {doc._.fileid}\")\n",
    "print(f\"Metadata: {doc._.metadata}\")\n",
    "print(f\"Tokens: {len(doc)}\")\n",
    "print(f\"Sentences: {len(doc.spans.get('ud_sents', []))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sents-header",
   "metadata": {},
   "source": [
    "## Sentences with Citations\n",
    "\n",
    "UD sentence boundaries are preserved in `doc.spans[\"ud_sents\"]`. Each span has:\n",
    "- `span._.citation` - the `sent_id` from the CoNLL-U file\n",
    "- `span._.metadata` - includes the original `# text = ...` comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate sentences with citations\n",
    "\n",
    "for sent in list(doc.spans[\"ud_sents\"])[:10]:\n",
    "    print(f\"{sent._.citation}: {sent.text[:60]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access sentence metadata\n",
    "\n",
    "sent = doc.spans[\"ud_sents\"][0]\n",
    "print(f\"Citation: {sent._.citation}\")\n",
    "print(f\"Metadata: {sent._.metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use ud_sents() for convenient iteration across files\n",
    "\n",
    "from itertools import islice\n",
    "\n",
    "for sent in islice(reader.ud_sents(), 5):\n",
    "    print(f\"{sent._.citation}: {sent.text[:70]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ud-header",
   "metadata": {},
   "source": [
    "## UD Annotations (token._.ud)\n",
    "\n",
    "All 10 CoNLL-U columns are preserved in `token._.ud`:\n",
    "- `id`, `form`, `lemma`, `upos`, `xpos`\n",
    "- `feats` (parsed dict), `head`, `deprel`, `deps`, `misc` (parsed dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine token UD annotations\n",
    "\n",
    "token = doc[0]\n",
    "print(f\"Token: {token.text}\")\n",
    "print()\n",
    "print(\"UD annotations (token._.ud):\")\n",
    "pprint(token._.ud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare UD data with spaCy attributes\n",
    "# Both are populated from the gold UD annotations\n",
    "\n",
    "print(f\"{'Token':<12} {'lemma_':<12} {'pos_':<8} {'dep_':<10} {'ud[feats]'}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for token in doc[:10]:\n",
    "    feats = token._.ud.get('feats', {})\n",
    "    feats_str = ', '.join(f\"{k}={v}\" for k, v in feats.items()) if feats else '-'\n",
    "    print(f\"{token.text:<12} {token.lemma_:<12} {token.pos_:<8} {token.dep_:<10} {feats_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access morphological features\n",
    "\n",
    "print(\"Tokens with Case feature:\")\n",
    "for token in doc[:20]:\n",
    "    feats = token._.ud.get('feats', {})\n",
    "    if 'Case' in feats:\n",
    "        print(f\"  {token.text}: {feats['Case']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spacy-header",
   "metadata": {},
   "source": [
    "## spaCy Integration\n",
    "\n",
    "Standard spaCy attributes are populated from UD data, so you can use familiar spaCy patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all proper nouns (named entities candidates)\n",
    "\n",
    "propn_tokens = [t for t in doc if t.pos_ == \"PROPN\"]\n",
    "print(f\"Proper nouns in document: {len(propn_tokens)}\")\n",
    "print()\n",
    "print(\"First 20:\")\n",
    "for t in propn_tokens[:20]:\n",
    "    print(f\"  {t.text} (sent: {t._.ud.get('id', '?')})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependency structure is preserved\n",
    "\n",
    "sent = doc.spans[\"ud_sents\"][0]\n",
    "print(f\"Sentence: {sent.text}\")\n",
    "print()\n",
    "print(f\"{'Token':<12} {'Head':<12} {'Deprel':<10}\")\n",
    "print(\"-\" * 35)\n",
    "for token in sent:\n",
    "    print(f\"{token.text:<12} {token.head.text:<12} {token.dep_:<10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unified-header",
   "metadata": {},
   "source": [
    "## LatinUDReader: All Treebanks at Once\n",
    "\n",
    "Use `LatinUDReader` to access multiple treebanks through a single interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a reader for specific treebanks\n",
    "# (Set auto_download=False to skip download prompts in demo)\n",
    "\n",
    "# unified = LatinUDReader(treebanks=[\"proiel\", \"perseus\"])\n",
    "# for sent in islice(unified.ud_sents(), 10):\n",
    "#     print(f\"{sent._.citation}: {sent.text[:60]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download all treebanks at once (run manually when ready)\n",
    "\n",
    "# LatinUDReader.download_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ner-header",
   "metadata": {},
   "source": [
    "## Use Case: Bootstrapping NER Datasets\n",
    "\n",
    "The UD reader is useful for bootstrapping NER annotation projects:\n",
    "1. Gold-standard tokenization and sentence boundaries\n",
    "2. PROPN tags as a **heuristic** for finding candidate sentences (not ground truth!)\n",
    "3. Morphological features may help with entity classification\n",
    "4. Sentence citations provide traceability back to source\n",
    "\n",
    "**Note:** PROPN â‰  named entity. This is a starting point for finding sentences worth annotating, not a labeled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find sentences containing proper nouns (candidates for annotation)\n",
    "# PROPN is a heuristic - these need human review!\n",
    "\n",
    "ner_candidates = []\n",
    "\n",
    "for sent in reader.ud_sents():\n",
    "    propns = [t for t in sent if t.pos_ == \"PROPN\"]\n",
    "    if propns:\n",
    "        ner_candidates.append({\n",
    "            'citation': sent._.citation,\n",
    "            'text': sent.text,\n",
    "            'propn_hints': [t.text for t in propns],  # hints, not labels\n",
    "        })\n",
    "\n",
    "print(f\"Sentences with PROPN tokens (candidates for annotation): {len(ner_candidates)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview candidates for annotation\n",
    "\n",
    "for item in ner_candidates[:10]:\n",
    "    print(f\"{item['citation']}\")\n",
    "    print(f\"  Text: {item['text'][:70]}...\")\n",
    "    print(f\"  PROPN hints: {item['propn_hints']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export candidates for annotation (e.g., to JSONL for Label Studio, Prodigy, etc.)\n",
    "\n",
    "import json\n",
    "\n",
    "# Sample export\n",
    "for item in ner_candidates[:3]:\n",
    "    print(json.dumps(item, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raw-header",
   "metadata": {},
   "source": [
    "## Raw Text Access\n",
    "\n",
    "Use `texts()` for raw strings with zero NLP overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw text iteration (reads from # text = comments)\n",
    "\n",
    "for text in islice(reader.texts(), 5):\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentences as strings\n",
    "\n",
    "for text in islice(reader.sents(as_text=True), 5):\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stats-header",
   "metadata": {},
   "source": [
    "## Corpus Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic stats for a treebank\n",
    "\n",
    "total_sents = 0\n",
    "total_tokens = 0\n",
    "\n",
    "for doc in reader.docs():\n",
    "    total_sents += len(doc.spans.get('ud_sents', []))\n",
    "    total_tokens += len(doc)\n",
    "\n",
    "print(f\"PROIEL Treebank Statistics:\")\n",
    "print(f\"  Files: {len(reader.fileids())}\")\n",
    "print(f\"  Sentences: {total_sents:,}\")\n",
    "print(f\"  Tokens: {total_tokens:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tag distribution\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "pos_counts = Counter()\n",
    "for doc in reader.docs():\n",
    "    for token in doc:\n",
    "        pos_counts[token.pos_] += 1\n",
    "\n",
    "print(\"POS Tag Distribution:\")\n",
    "for pos, count in pos_counts.most_common(15):\n",
    "    print(f\"  {pos:<8} {count:>8,}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
